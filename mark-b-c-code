/*
 * osbf_bayes.c
 * 
 * See Copyright Notice in osbflib.h
 * 
 */

#include "os.h"

#include <assert.h>
#include <stdio.h>
#include <ctype.h>
#include <float.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <math.h>
#include <sys/mman.h>
#include <inttypes.h>
#include <errno.h>

#define DEBUG 0			/* usefully 0, 1, 2, or > 2 */

#include "datutils.h"
#include "osbflib.h"
#include "tokendb.h"
#include "osbf_bayes.h"
#include "logging.h"
#include "trace.h"


#define TMPBUFFSIZE 512

char		tempbuf   [TMPBUFFSIZE + 2];

extern uint32_t	microgroom_displacement_trigger;
uint32_t	max_token_size = OSBF_MAX_TOKEN_SIZE;
uint32_t	max_long_tokens = OSBF_MAX_LONG_TOKENS;
uint32_t	limit_token_size = 0;
enum a_priori_options a_priori = LEARNINGS;

/*
 * the hash coefficient tables should be full of relatively prime
 * numbers, and preferably superincreasing, though both of those
 * are not strict requirements. The two tables must not have a
 * common prime.
 */

static uint32_t	hctable1[] = {1, 3, 5, 11, 23, 47, 97, 197, 397, 797};
static uint32_t	hctable2[] =
{7, 13, 29, 51, 101, 203, 407, 817, 1637, 3277};


/* maps strings to a_priori_options enum */
const char     *a_priori_strings[] = {
	"LEARNINGS",
	"INSTANCES",
	"CLASSIFICATIONS",
	"MISTAKES",
	NULL
};

/*****************************************************************/
/* experimental code */
#if (0)
static double 
lnfact(uint32_t n)
{
	static double	lnfact_table[1001];

	if (n <= 1)
		return 0.0;
	if (n <= 1000)
		return lnfact_table[n] ?
		    lnfact_table[n] : (lnfact_table[n] = lgamma(n + 1.0));
	else
		return lgamma(n + 1.0);
}

static double 
conf_factor(uint32_t n, uint32_t k, double interval)
{
	uint32_t	i, j, start, end;
	double		b, sum;

	j = floor(0.5 + interval * n);

	if (j > k)
		start = 0;
	else
		start = k - j;

	if (j + k <= n)
		end = j + k;
	else
		end = n;

	sum = 0;
	for (i = start; i <= end; i++) {
		b = exp(lnfact(n) - lnfact(i) - lnfact(n - i) - n * log(2));
		if (sum + b < 1)
			sum += b;
	}

	return 1 - sum;
}
#endif

/*****************************************************************/

/*----------------------------------------------------------------
   Returns full string passed in (ie, p_text). Loads p_token with
   the length of next token.
   
   	max_p points to '\0' that terminates p_text 
   	delims is string of delimiter characters (cannot be NULL)


 -----------------------------------------------------------------*/

unsigned char  *
get_next_token(unsigned char *p_text,
    unsigned char *max_p,
    const char *delims,
    uint32_t *p_toklen)
{
	unsigned char	*p_ini;	
	unsigned char	*lim;	

#if TRACE_TOKEN_PARSE
	/*
		XXX: not sure about signed vs. unsigned here.

		Notes on signed versus unsigned charaters:

		(1)	An unsigned character that is bigger than that
			largest possible signed value cannot be
			converted reliably--I believe the behavior is
			undefined in the standard.

		(2)	A negative signed character will be converted
			into an unsigned character by repeatedly adding
			(or subtracting) UCHAR + 1, so getting back to
			the original value (by casting back) is not 
			possible.

		(3)	Tim Bray, in his article on UTF8, says functions
			link strcpy() work just fine with UTF8 strings.
			So I'm not going to worry about this now.  Will
			need to write some unit tests to work it out.
	*/

	char		tracebuf[10] = {0};
	(void) strlcpy(tracebuf, (char *) p_text, sizeof(tracebuf));
	log_debug(0, "TRACE_TOKEN_PARSE: get_next_token: "
	    "entering: p_text[10]='%s', max_p - p_text = %td, "
	    "delims = '%s', *p_toklen = %" PRIu32, 
	    tracebuf, max_p - p_text, delims, *p_toklen);
#endif

#define DELIMP(P) (!isgraph((int) *(P)) || strchr (delims, (int) *(P)))

	while (p_text < max_p && DELIMP(p_text))
		p_text++;
	p_ini = p_text;

	lim = max_p;
	if (limit_token_size && p_ini + max_token_size < max_p) {
#if TRACE_TOKEN_PARSE
		log_debug(0, "TRACE_TOKEN_PARSE: get_next_token: "
		    "hit long token, returning first %" PRIu32 " chars",
		    max_token_size);
#endif
		lim = p_ini + max_token_size;
	}

	/*-------------------------------------------------------------
	 * 	Long tokens are "reconstituted" in get_next_hash 
	 * 	(their hashes are XOR'd).
	 *------------------------------------------------------------*/

	while (p_text < lim && !DELIMP(p_text))
		p_text++;

	*p_toklen = p_text - p_ini;
#if TRACE_TOKEN_PARSE
	log_debug(0, "TRACE_TOKEN_PARSE: get_next_token: "
	    "exiting with *p_toklen = %" PRIu32, *p_toklen);
#endif

	return p_ini;
}

/*---------------------------------------------------------------------
 * Return 0 on success, 1 on error.
 *
 * Parse next token in pts->ptok, load pts->toklen with length of next
 * token, and load pts->hash with hash of next token.
 *
 * pts->delims defines the delimiters used to define what a token is.
 *
 * pts->ptok is set to first non-delim character in the string that is
 * getting parsed; that is, if, on entry, pts->ptok points to the space
 * in " abc", on exit it will point to the "a".
 *
 *--------------------------------------------------------------------*/

uint32_t 
get_next_hash(struct token_search *pts)
{
	uint32_t	hash_acc = 0;
	uint32_t	count_long_tokens = 0;
	int		rval;
#if TRACE_LEARN
	char		*tracebuf;
#endif

	rval = EXIT_SUCCESS;

	pts->ptok += pts->toklen;
	pts->ptok = get_next_token(pts->ptok, pts->ptok_max,
	    pts->delims, &(pts->toklen));

#ifdef OSBF_MAX_TOKEN_SIZE
	/*-------------------------------------------------------------
	 * long tokens, probably encoded lines 
	 *
	 * If we hit a token longer than 60 characters,
	 *
	 * 	0. Initialize current hash to zero.
	 *
	 * 	1. Set current hash equal to current hash, XOR'd with
	 * 	   tokens hash.
	 *
	 * 	2. Get the next token.  
	 *
	 * 	3. If next token is long, and we have hit less than
	 * 	   1,000 long tokens in a row, goto step 1.
	 *
	 * 	4. Set current hash equal to current hash, XOR'd with
	 * 	   tokens hash.
	 *
	 ------------------------------------------------------------*/

	while (pts->toklen >= max_token_size
	    && count_long_tokens < max_long_tokens) {
		count_long_tokens++;
		/* XOR new hash with previous one */
		hash_acc ^= strnhash(pts->ptok, pts->toklen);
		pts->ptok += pts->toklen;
		pts->ptok = get_next_token(pts->ptok, pts->ptok_max,
		    pts->delims, &(pts->toklen));
	}
#endif

	if (pts->toklen > 0 || count_long_tokens > 0) {
		hash_acc ^= strnhash(pts->ptok, pts->toklen);
		pts->hash = hash_acc;
		/*
		 * fprintf(stderr, " %0lX %lu\n", hash_acc,
		 * pts->toklen);
		 */
	}
	else {
		/* no more hashes */
		/*
		 * fprintf(stderr, "End of text %0lX %lu\n",
		 * hash_acc, pts->toklen);
		 */
		rval = EXIT_FAILURE;
	}

#if TRACE_LEARN
	tracebuf = (char *) calloc(pts->toklen + 1, sizeof(char));
	strncpy(tracebuf, (char *) pts->ptok, pts->toklen);
	log_debug(0, "TRACE_LEARN: get_next_hash: rval = %d, "
	    "token = '%s', hash = %" PRIu32,
	    rval, tracebuf, pts->hash);
	free(tracebuf);
	tracebuf = NULL;
#endif

	return (rval);
}

static void
update_bucket(CLASS_STRUCT *class, uint32_t hashpipe[], int sense)
{
	uint32_t	bindex;
	uint32_t	h1, h2;
	uint32_t	window_idx;

#if TRACE_LEARN
	int		trace_i;

	log_debug(0, "TRACE_LEARN: update_bucket: hashpipe contents: ");
	for (trace_i = 0; trace_i < OSB_BAYES_WINDOW_LEN; trace_i++)
		log_debug(0, "TRACE_LEARN:     %d: %" PRIu32,
		    trace_i, hashpipe[trace_i]);
#endif

	for (window_idx = 1; window_idx < OSB_BAYES_WINDOW_LEN; window_idx++) {

		h1 = hashpipe[0] * hctable1[0] +
		    hashpipe[window_idx] * hctable1[window_idx];
		h2 = hashpipe[0] * hctable2[0] +
		    hashpipe[window_idx] * hctable2[window_idx];

		save_bigram(hashpipe[0], hashpipe[window_idx], h1, h2);

#if TRACE_LEARN
		log_debug(0, "TRACE_LEARN: update_bucket: hash pair: %"
		    PRIu32 " / %" PRIu32, hashpipe[0], hashpipe[window_idx]);
#endif

		bindex = FAST_FIND_BUCKET(class, h1, h2);
#if TRACE_LEARN
		log_debug(0, "TRACE_LEARN: update_bucket: bindex=%" PRIu32
		    ", from h1: %12" PRIu32 ", h2: %12" PRIu32, 
		    bindex, h1, h2);
#endif
		if (bindex < class->header->num_buckets) {
			if (BUCKET_IN_CHAIN(class, bindex)) {
				if (!BUCKET_IS_LOCKED(class, bindex))
					osbf_update_bucket(class, bindex,sense);
			}
			else if (sense > 0) {
				osbf_insert_bucket(class, bindex, h1, h2,sense);
			}
		}
		else 
			/*
			 * We're full, so training fails, but don't make this
			 * fatal--maybe existing training is already good
			 * enough.
			 */

			log_warnx("osbf_bayes_train: .cfc file %s is "
			    "full, can't add more tokens.", class->classname);
	}
}


/******************************************************************/
/* Train the specified class with the text pointed to by "p_text" */
/******************************************************************/
void 
osbf_bayes_train(
    const unsigned char	*p_text,	/* pointer to text */
    unsigned long	 text_len,	/* length of text */
    const char		*delims,	/* token delimiters */
    CLASS_STRUCT	*class,		/* database to be trained */
    int			 sense,		/* 1 => learn;  -1 => unlearn */
    enum learn_flags	 flags)
{
	int32_t		learn_error;
	int32_t		i;
	uint32_t	hashpipe[OSB_BAYES_WINDOW_LEN + 1];

	/*
	 * on 5000 msgs from trec06, average number of tokens
	 * (including sentinels at ends) is 150; 2/3 of msgs are
	 * under 150; 80% are under 200; 90% are under 300.  99%
	 * are under 1000.  So if one were to make a copy rather
	 * than pipelining, 200 would seem to be a good starting
	 * length
	 */

	int32_t		num_hash_paddings;
	int		microgroom;
	struct token_search ts;

	log_errx_unless(delims != NULL,
	    "NULL delimiters; use empty string instead");

	ts.ptok = (unsigned char *)p_text;
	ts.ptok_max = (unsigned char *)(p_text + text_len);
	ts.toklen = 0;
	ts.hash = 0;
	ts.delims = delims;

	if (class->state == OSBF_CLOSED)
		log_errx("Trying to train a closed class\n");
	if (class->usage != OSBF_WRITE_ALL)
		log_errx("Trying to train class %s without opening for write",
		    class->classname);

	microgroom = (flags & NO_MICROGROOM) == 0;
	memset(class->bflags, 0,
	    class->header->num_buckets * sizeof(unsigned char));

	/* init the hashpipe with 0xDEADBEEF  */
	for (i = 0; i < OSB_BAYES_WINDOW_LEN; i++)
		hashpipe[i] = DEADBEEF;

	learn_error = 0;
	/*
	 * experimental code - set num_hash_paddings = 0 to
	 * disable
	 */
	/* num_hash_paddings = OSB_BAYES_WINDOW_LEN - 1; */
	num_hash_paddings = OSB_BAYES_WINDOW_LEN - 1;

	while (learn_error == 0 && ts.ptok <= ts.ptok_max) {

		/* Returns failure when no more tokens to parse. */
		if (get_next_hash(&ts) == EXIT_FAILURE) {

			/*
			 * After eof, insert fake tokens until the last
			 * real token comes out at the other end of the
			 * hashpipe.
			 */

			if (num_hash_paddings-- > 0)
				ts.hash = DEADBEEF;
			else
				break;
		}

		/*
		 * Shift the hash pipe down one and insert new
		 * hash.
		 */

		for (i = OSB_BAYES_WINDOW_LEN - 1; i > 0; i--)
			hashpipe[i] = hashpipe[i - 1];

		hashpipe[0] = ts.hash;

		/*
			XXX: Arg2 signed v. unsigned?
		*/

		save_token((char *) ts.ptok, ts.toklen, ts.hash);

		update_bucket(class, hashpipe, sense);
	}
	write_tokenbuf();
	write_bigrambuf();

	if (sense > 0) {
		/* extra learnings are all those done with the  */
		/* same document, after the first learning */
		if (flags & EXTRA_LEARNING) {
			/* increment extra learnings counter */
			class->header->extra_learnings += 1;
		}
		else {
			/* increment normal learnings counter */

			/*
			 * old code disabled because the
			 * databases are disjoint and this
			 * correction should be applied to both
			 * simultaneously
			 * 
			 * class->header->learnings += 1; if
			 * (class->header->learnings >=
			 * OSBF_MAX_BUCKET_VALUE) { uint32_t i;
			 * 
			 * class->header->learnings >>= 1; for (i =
			 * 0; i < NUM_BUCKETS (class); i++)
			 * BUCKET_VALUE (class, i) = BUCKET_VALUE
			 * (class, i) >> 1; }
			 */

			if (class->header->learnings < OSBF_MAX_BUCKET_VALUE) {
				class->header->learnings += 1;
			}

			/* increment false negative counter */
			if (flags & FALSE_NEGATIVE) {
				class->header->false_negatives += 1;
			}
		}
	}
	else {
		if (flags & EXTRA_LEARNING) {
			/* decrement extra learnings counter */
			if (class->header->extra_learnings > 0)
				class->header->extra_learnings -= 1;
		}
		else {
			/* decrement learnings counter */
			if (class->header->learnings > 0)
				class->header->learnings -= 1;
			/* decrement false negative counter */
			if ((flags & FALSE_NEGATIVE) && \
			    class->header->false_negatives > 0)
				class->header->false_negatives -= 1;
		}
	}

#if 0
	{
		unsigned	n = 40;
		unsigned	j;

		fprintf(stderr, "### %u nonzero buckets after training =", n);
		for (j = 0; j < NUM_BUCKETS(class); j++)
			if (BUCKET_IN_CHAIN(class, j)) {
				fprintf(stderr, " %u", j);
				if (--n == 0)
					break;
			}
		fprintf(stderr, "\n");
	}
#endif

}

/*
	Initialize token parsing structure with text to parse.
*/

static void
init_tokensearch(
    struct token_search *tsp,
    unsigned char	*text,
    unsigned long	 text_sz,
    const char		*delims)
{
	tsp->ptok = text;
	tsp->ptok_max = text + text_sz;
	tsp->toklen = 0;
	tsp->hash = 0;
	tsp->delims = delims;
}

/*
	I think these weights have something to do with the following:

	A Markovian classifier operates on the concept that _patterns_
	of words are far more important than individual words.

	For example, a Bayesian encountering the phrase "the quick brown
	fox jumped" would have five features: "the", "quick", "brown",
	"fox", and "jumped".

	A Sparse Binary Polynomial Hasher would have sixteen features:

		 the
		 the quick
		 the <skip> brown
		 the quick brown
		 the <skip> <skip> fox
		 the quick <skip> fox
		 the <skip> brown fox
		 the quick brown fox

	... and so on.	But each of these features would recieve the
	same weighting in the Bayesian chain rule above.

	The change to become a Markovian is simple--instead of giving
	each Sparse Binary Polynomial Hash (SBPH) feature a weight of 1,
	give each feature a weight corresponding to how long a Markov
	Chain it matches in either of the archetype texts.

	A simple way to do this would be to make the weight equal to
	the number of words matched - in this case the weights would be:

		 the				1
		 the quick			2
		 the <skip> brown		2
		 the quick brown		3
		 the <skip> <skip> fox		2
		 the quick <skip> fox		3
		 the <skip> brown fox		3
		 the quick brown fox		4

	and indeed, this gives some improvement over standard SBPH.

	But there is room for further improvement.  The filter as
	stated above is still a linear filter; it cannot learn (or even
	express!) anything of the form:

		"A" or "B" but not both

	This is a basic limit discovered by Minsky and Papert in 1969
	and published in _Perceptrons_.

	In this particular case there is a convenient way to work around
	this problem.  The solution is to make the weights of the terms
	"superincreasing", such that long Markov chain features have so
	high a weight that shorter chains are completely overruled.

	For example, if we wanted to do "A or B but not both" in such
	a superincreasing filter, the weights:

		"A" at 1 
		"B" at 1 
		"A B" at -4

	will give the desired results.

	For convenience in calculation, CRM114 uses the superincreasing
	weights defined by the series 2^(2n)---that is,

		 the				1
		 the quick			4
		 the <skip> brown		4
		 the quick brown		16
		 the <skip> <skip> fox		4
		 the quick <skip> fox		16
		 the <skip> brown fox		16
		 the quick brown fox		64

	Note that with these weights, a chain of length N can override
	all chains of length N-1, N-2, N-3... and so on.

	This Markovian matching gives a considerable increase in accuracy
	over SBPH matching, and almost a factor of 2 improvement over
	Bayesian matching.

	    -Bill Yerazunis
	
*/

static void
set_weights(
    double		 weight[],
    int			 weight_n,
    CLASS_STRUCT	*classes[],
    unsigned		 cn)
{
	double		  exponent;
	uint32_t	  total, i;
	CLASS_STRUCT	**p, **pn;

	assert(weight_n == 6);

	/* 
		Default weights are set empirically, based on experience, to:

			(5 - d) ^ (5 - d),  
		where 

			d = number of skipped tokens in the sparse bigram.

		The closer the tokens, the (much) higher the weight.
	 */

	weight[0] = 0;
	weight[1] = 3125;	/* 5^5 (d = 0) */
	weight[2] = 256;	/* 4^4 (d = 1) */
	weight[3] = 27;		/* 3^3 (d = 2) */
	weight[4] = 4;		/* 2^2 (d = 3) */
	weight[5] = 1;		/* 1^1 (d = 4) */

	/*
		But if we have a low number of over all learnings,
		we set weights 1, 2, 3, and 4 a different way.
	*/

	pn = classes + cn;

	total = 0;
	for (p = classes; p < pn; p++) {

		i = (*p)->header->learnings;

		/* 
			Avoid division by 0.
		*/

		if (i == 0)
			i++;

		total += i;
	}

	exponent = pow(total * 3, 0.2);

	if (exponent < 5) {
		weight[1] = pow(exponent, exponent);
		weight[2] = pow(exponent*4.0/5.0, exponent*4.0/5.0);
		weight[3] = pow(exponent*3.0/5.0, exponent*3.0/5.0);
		weight[4] = pow(exponent*2.0/5.0, exponent*2.0/5.0);
	}

}

/*
	Load ptt with # of times each class has been trained.

	Also does one thing I'm not sure about:

		copies trainings from class header into class

	For now, keep these things in as I'm copying code.
*/

static void
load_trainingcounts(
    CLASS_STRUCT	*classes[],
    unsigned		 cn,
    uint32_t		 ptt[])
{
	CLASS_STRUCT	**p, **pn;
	int		  ci;

	pn = classes + cn;
	for (p = classes; p < pn; p++) {
		
		ci = (int) (p - classes);

		(*p)->learnings = (*p)->header->learnings;

		ptt[ci] = (*p)->header->learnings;

		/* 
			Avoid division by 0.
		*/

		if ((*p)->learnings == 0)
			(*p)->learnings++;
	}
}


/*
	Clear each class's accumulators and flags.
*/

static void
init_classes(CLASS_STRUCT *classes[], unsigned cn)
{
	CLASS_STRUCT	**p, **pn;

	pn = classes + cn;
	for (p = classes; p < pn; p++) {

		memset((*p)->bflags, 0,
		    (*p)->header->num_buckets * sizeof(unsigned char));

		/* 
			absolute hit counts 
		*/

		(*p)->hits = 0.0;	

		/* 
			absolute hit counts 
		*/

		(*p)->totalhits = 0;	

		/* 
			features counted per class
		*/

		(*p)->uniquefeatures = 0;	

		/* 
			missed features per class
		*/

		(*p)->missedfeatures = 0;	

	}
}

/*
	Compute initial likelihood of text belonging to a given class
	based on meta data for the class; for example, the number of
	trainings that class has had.

	If a class has had more documents assigned to it, then a-priori
	we might expect that the next text we process will be more likely
	to be in that class.

	Which metric is used to set this probability is controled by a
	global variable initialized above in this module.
*/

static void
init_classprobabilities(
    CLASS_STRUCT	*classes[],
    unsigned		 cn,
    double		 ptc[])
{
	char		 *f = "set_a_priori";
	double		  a_priori_counter[OSBF_MAX_CLASSES];
	double		  total_a_priori;
	int		  ci;
	CLASS_STRUCT	**p, **pn;

	/* 
		The class probability based on learnings (or 
		classifications, or ...).
	*/

	total_a_priori = 0;
	pn = classes + cn;
	for (p = classes; p < pn; p++) {

		ci = (int) (p - classes);

		/* 
			Select type of estimate for a-priori.
		*/

#if TRACE_CLASSIFY
		log_debug(0, "TRACE_CLASSIFY: %s: using %s for a-priori "
		    "estimate", 
		    f, a_priori_strings[a_priori]);
#endif
		switch (a_priori) {
		case LEARNINGS:
			a_priori_counter[ci] = (*p)->header->learnings;
			break;
		case INSTANCES:
			if ((*p)->header->db_version >= OSBF_DB_FP_FN_VERSION)
				a_priori_counter[ci] =
				    (*p)->header->classifications +
				    (*p)->header->false_negatives -
				    (*p)->header->false_positives;
			else
				log_errx("Database version %s doesn't support "
				    "'INSTANCES' for a priori estimation. Try "
				    "'CLASSIFICATIONS' instead.",
				    (*p)->header->db_version);
			break;
		case CLASSIFICATIONS:
			a_priori_counter[ci] = (*p)->header->classifications;
			break;
		case MISTAKES:
			a_priori_counter[ci] = (*p)->header->false_negatives;
			break;
		default:
			log_errx("%s: the a-priori option (%d) is out of range "
			    "[%d, %d]",
			    f, a_priori, 0, A_PRIORI_UPPER_LIMIT - 1);
			break;
		}

		/* 
			Avoid division by zero.
		*/

		if (a_priori_counter[ci] < 1)
			a_priori_counter[ci] = 1;

		total_a_priori += a_priori_counter[ci];
	}

	for (p = classes; p < pn; p++) {
		ci = (int) (p - classes);
		ptc[ci] = a_priori_counter[ci] / total_a_priori;
	}
}

/*
	Confidence factors

	The motivation for confidence factors is to reduce the
	noise introduced by features with small counts and/or low
	significance. 

	This is an attempt to mimic what we do when inspecting a message
	to tell if it is spam or not. We intuitively consider only a few
	tokens, those which carry strong indications, according to what
	we've learned and remember, and discard the ones that may occur
	(approximately) equally in both classes.

	Once P(Feature|Class) is estimated, the calculated feature
	probabilities are adjusted using the following formula:

		CP(Feature|Class) = 1/num_classes + 
		CF(Feature) * (P(Feature|Class) - 1/num_classes)

	Where CF(Feature) is the confidence factor and
	CP(Feature|Class) is the adjusted estimate for the
	probability.

	CF(Feature) is calculated taking into account the
	weight, the max and the min frequency of the feature
	over the classes, using the empirical formula:

	ConfidenceFactor(Feature) =	  

	  (Hmax - Hmin)^2 + Hmax*Hmin - K1/SH
	(--------------------------------------) ^ K2
			   SH^2
	----------------------------------------------------
		 1 +  K3 / (SH * Weight)

	where:

	  Hmax	- Number of documents with the feature
	  "F" on the class with max local probability;

	  Hmin	- Number of documents with the feature
	  "F" on the class with min local probability;

	  SH - Sum of Hmax and Hmin 

	  K1, K2, K3 - Empirical constants

	OBS: 

	- Hmax and Hmin are normalized to the max number of
	learnings of the 2 classes involved.

	- Besides modulating the estimated P(Feature|Class),
	reducing the noise, 0 <= CF < 1 is also used to restrict
	the probability range, avoiding the certainty falsely
	implied by a 0 count for a given class.

	-- Fidelis Assis
*/

static double
confidence_factor(CLASS_STRUCT *cmin, CLASS_STRUCT *cmax, double weight)
{
	uint32_t	maxhits, minhits, sum_hits;
	int32_t		diff_hits;
	double		cfx = 1;
	double		K1, K2, K3; 

	/*
		Constants.
	*/

	K1 = 0.25;
	K2 = 12;
	K3 = 8;

	/*
	 	Normalize hits to max learnings.
	 */

	minhits = cmin->hits;
	maxhits = cmax->hits;
	if (cmin->learnings < cmax->learnings)
		minhits *= (double)cmax->learnings / (double)cmin->learnings;
	else
		maxhits *= (double)cmin->learnings / (double)cmax->learnings;

	sum_hits = maxhits + minhits;
	diff_hits = maxhits - minhits;
	if (diff_hits < 0)
		diff_hits = -diff_hits;

	/* 
		XXX: MKB: This code looks wrong--what is NO_EDDC? 
		Commenting it out and just using second branch.

	if (flags & NO_EDDC)	// || p_featmin > 0 )
		confidence_factor = 1 - OSBF_DBL_MIN;
	else
		cfx = 0.8 + (cmin->header->learnings +
		    cmax->header->learnings) / 20.0;
	*/

	cfx = 0.8 + (cmin->header->learnings + cmax->header->learnings) / 20.0;

	if (cfx > 1)
		cfx = 1;

	/*
		Ask Fidelis: mean to mix normalized and un-normalized
		hits here?
	*/

	return cfx *
	    pow(
	        (diff_hits * diff_hits - K1 / (cmax->hits + cmin->hits)) 
	            / 
	            (sum_hits * sum_hits), 
	        2) 
	    /
	    (1.0 + K3 / ((cmax->hits + cmin->hits) * weight)
	    );
}

/*
	Return 1 if this feature is signifcant enough to warrant
	recalculating class probabilities.

	Important side-effects:

		- accumulate feature hits in the class, and

		- record classes with max and min feature probabilities.

	Feature probability is:

			# of trainings with this feature
		      ------------------------------------
			     total # of trainings
*/

static int
significant_feature(
    CLASS_STRUCT	*classes[],
    unsigned int	 cn, 
    uint32_t		 token1_hash,
    uint32_t		 token2_hash,
    double		 toosmall,
    int			*i_featmin,
    int			*i_featmax)
{
	double		  p_feat, p_featmin, p_featmax;
	int		  already_seen;
	int		  ci;
	uint32_t	  hindex, lh, lh0;
	CLASS_STRUCT	**pp, **ppn, *p;

	hindex = token1_hash;

	/*
		Initialize state.
	*/

	p_featmin = 1.0;
	p_featmax = 0;
	p_feat = 0;
	*i_featmin = *i_featmax = 0;
	already_seen = 0;

	ppn = classes + cn;
	for (pp = classes; pp < ppn; pp++) {
		p = *pp;
		ci = (int) (pp - classes);

		lh0 = HASH_INDEX(p, hindex);
		p->hits = 0;

		/*
			Look for feature with token1_hash and token2_hash.
		*/

		lh = FAST_FIND_BUCKET(p, token1_hash, token2_hash);

		/*
			Truth Table for feature conditions

			   valid bckt     Y  Y  Y  Y  N  N  N  N
			   unseen feat    Y  Y  N  N  Y  Y  N  N
			   bckt in chain  Y  N  Y  N  Y  N  Y  N
			-----------------------------------------
			        case:     a  b  c  c  b  b  b  b

			case a: update prob(feature|p), mark
				feature as seen (in this p), and
				update min/max probabilities if we have
				new winners.

			case b: set p(feature|p) = 0, mark this
				p as having min. probability for
				this feature, and set min probability to
				zero.

			case c: set already_seen flag to true.

		*/

		/*
			The bucket is valid if its index is valid. 

			If the index "lh" is >= the number of buckets,
			the .cfc file is full, the bucket wasn't found,
			and we could not return the index of the next
			empty bucket, as there are none.
		 */

		if (VALID_BUCKET(p, lh) && 
		    BUCKET_FLAGS(p, lh) == 0 && 
		    BUCKET_IN_CHAIN(p, lh)) {

			/* 
				Mark the feature as seen in this p.

				Only paying attention to the  first
				instance of a feature makes the code
				both faster and more accurate.
			*/

			p->bflags[lh] = 1;	
			p->uniquefeatures += 1;
						
			p->hits = BUCKET_VALUE(p, lh);
			p->totalhits += p->hits;

			p_feat = p->hits / p->learnings;

			if (p_feat <= p_featmin) {
				*i_featmin = ci;
				p_featmin = p_feat;
			}
			if (p_feat >= p_featmax) {
				*i_featmax = ci;
				p_featmax = p_feat;
			}
		}


		/*

		  Either bucket is invalid or it is not in a chain.

		  An invalid bucket is treated like a feature not found.

		  A feature that wasn't found can't be marked as already
		  seen in the doc because the index lh doesn't refer
		  to it, but to the first empty bucket after the chain.
		  Thus, all not-found features in the same chain will have
		  the same value for lh. 

		  This is not a problem though, because if the feature
		  is found in another p, it'll be marked as seen
		  on that p, which is enough to mark it as seen. If
		  it's not found in any p, it will have zero count
		  on all classes and will be ignored as well. So, only
		  found features are marked as seen.

		 */

		else if (!VALID_BUCKET(p, lh)
		    || BUCKET_FLAGS(p, lh) == 0) {
			*i_featmin = ci;
			p_featmin = p_feat = 0;

			/*
			 	For statistics only (for now...)
			*/

			p->missedfeatures += 1;
		}

		/* 
			Bucket is valid, flags not zero.
		*/

		else
			already_seen = 1;
	}
	/* 
		Ignore features that:

			- have been already seen,

			- are insignificant 

		Insignificant means either that the difference between
		the min and max probabilities was very small, or that
		the min divided into the max was lower than the limit
		specified by the caller.
	*/

	if (already_seen || p_featmax - p_featmin < 1E-6)
		return 0;
	if (p_featmin > 0 && p_featmax / p_featmin < toosmall)
		return 0;

	return 1;
}

/*************************************************************

	Update the probability of getting this feature, given 
	a class---p(Feature|Class)---using Bayes:

	XXX: MKB Check this ...

					P(S) P(S|F)
		P(F|S) =	-------------------------------
				  P(F|S) P(S) + P(F|H) P(H)

		S = class spam; 
		H = class ham; 
		F = feature

	Here we adopt a different method for estimating P(F|S). Instead
	of estimating P(F|S) as

			 hits[S][F]
		----------------------------	,
		  hits[S][F] + hits[H][F]

	like in the original code, we use 

			 hits[S][F] 
		----------------------------   ,
			learnings[S]

	which is the ratio between the number of messages of the class S
	where the feature F was observed during learnings and the total
	number of learnings of that class. Both values are kept in the
	respective .cfc file, the number of learnings in the header and
	the number of occurrences of the feature F as the value of its
	feature bucket.

	It's worth noting another important difference here: as we want
	to estimate the *number of messages* of a given class where a
	certain feature F occurs, we count only the first occurrence of
	each feature in a message (repetitions are ignored), both when
	learning and when classifying.

	Advantages of this method, compared to the original:

		- First of all, and the most important: accuracy is really
		much better, at about the same speed! With this higher
		accuracy, it's also possible to increase the speed, at
		the cost of a low decrease in accuracy, using smaller
		.cfc files;

		- It is not affected by different sized classes because
		the numerator and the denominator belong to the same
		class;

		- It allows a simple and fast pruning method that seems
		to introduce little noise: just zero features with lower
		count in a overflowed chain, zeroing first those in
		their right places, to increase the chances of deleting
		older ones.

	Disadvantages:

		- It breaks compatibility with previous .css file format
		because of different header structure and meaning of
		the counts.

	-- Fidelis Assis
 
*/

/*
	Adjust feature probabilities using the following formula:

		CP(Feature|Class) = 1/num_classes + 
		CF(Feature) * (P(Feature|Class) - 1/num_classes)

	and multiply with current class probability to get overall class
	probability after processing this feature.
*/

static double
reduce_noise(
    CLASS_STRUCT	*classes[],
    unsigned int	 cn,
    double		 a_priori,
    double		 factor,
    double		 ptc[]
    )
{
	double		renorm;
	unsigned int	ci;

	/*
		 Calculate the numerators - P(F|C) * P(C)
	*/

	renorm = 0.0;

	for (ci = 0; ci < cn; ci++) {

		/*
		 	P(C) = learnings[k] / total_learnings 

		 	P(F|C) = hits[k]/learnings[k],

			adjusted by the confidence factor.
		 */

		ptc[ci] = ptc[ci] *
		    (a_priori + factor *
		    (classes[ci]->hits / classes[ci]->learnings -
		    a_priori));

		if (ptc[ci] < OSBF_SMALLP)
			ptc[ci] = OSBF_SMALLP;

		renorm += ptc[ci];
	}

	return renorm;
}

/**********************************************************/
/* Given the text pointed to by "p_text", for each class  */
/* in the array "classes", find the probability that the  */
/* text belongs to that class                             */
/**********************************************************/

void 
osbf_bayes_classify(
    const unsigned char	*p_text,	/* pointer to text */
    unsigned long	 text_len,	/* length of text */
    const char		*delims,	/* token delimiters */
    CLASS_STRUCT	*classes[],	/* hash file names */
    unsigned int	 num_classes, 
    uint32_t		 flags,		/* flags */
    double		 toosmall,	/* Ignore features where:
    						pmax/pmin < toosmall */
/* returned values */
    double		 ptc[],		/* class probs */
    uint32_t		 ptt[]		/* number trainings per class */
)
{
	char		*f = "osbf_bayes_classify";
	struct token_search ts;
	uint32_t	hashpipe[OSB_BAYES_WINDOW_LEN + 1];
	double		factor;
	CLASS_STRUCT  **class_lim = classes + num_classes;
	CLASS_STRUCT  **pclass;
	int32_t		i;	/* we use i for our hashpipe
				 * counter, as needed. */
	double		 renorm;
	double		 a_priori_prob;	
	double		 feature_weight[6];

	unsigned int	 ci;
	int		 i_featmin, i_featmax;
	uint32_t	 h1, h2;
	int32_t		 hash_i;

	/*
		Validate.
	*/

	log_errx_unless((flags & COUNT_CLASSIFICATIONS) == 0,
	    "Asked to count classifications, but this must now be "
	    "done as a separate operation");

	log_errx_unless(delims != NULL,
	    "NULL delimiters; use empty string instead");

	log_errx_unless(text_len > 0, "Attempt to classify an empty text.");
	log_errx_unless(num_classes > 0, "At least one class must be given.");

	for (pclass = classes; pclass < class_lim; pclass++)
		if ((*pclass)->state == OSBF_CLOSED)
			log_errx("%s: class number %d is closed",
			    f, (int) (pclass - classes));

	/*
		Setup.
	*/

	init_tokensearch(&ts, (unsigned char *) p_text, text_len, delims);

	set_weights(feature_weight,
	    sizeof(feature_weight)/sizeof(feature_weight[0]), classes,
	    num_classes);

	init_classes(classes, num_classes);

	load_trainingcounts(classes, num_classes, ptt);

	a_priori_prob = 1.0 / (double) num_classes;
	init_classprobabilities(classes, num_classes, ptc);


	/*
		Init the hashpipe with 0xDEADBEEF.
	*/

	for (i = 0; i < OSB_BAYES_WINDOW_LEN; i++)
		hashpipe[i] = DEADBEEF;

	renorm = 0.0;

	/*
		Loop through each token pair, updating probability of
		each class producing these token pairs by multiplying.
	*/

	while (ts.ptok <= ts.ptok_max && get_next_hash(&ts) == 0) {

		/*
		 * Shift the hash pipe down one and insert new
		 * hash
		 */

		memmove(hashpipe + 1, hashpipe,
		    sizeof(hashpipe) - sizeof(hashpipe[0]));
		hashpipe[0] = ts.hash;
		ts.hash = 0;	/* clean hash */

		for (hash_i = 1; hash_i < OSB_BAYES_WINDOW_LEN; hash_i++) {
			h1 = hashpipe[0] * hctable1[0] +
			    hashpipe[hash_i] * hctable1[hash_i];
			h2 = hashpipe[0] * hctable2[0] +
			    hashpipe[hash_i] * hctable2[hash_i];

			if (!significant_feature(classes, num_classes, h1, h2,
			    toosmall, &i_featmin, &i_featmax))
				continue;

			factor = confidence_factor(classes[i_featmin],
			    classes[i_featmax], feature_weight[hash_i]);

			renorm = reduce_noise(classes, num_classes,
			    a_priori_prob, factor, ptc);

			for (ci = 0; ci < num_classes; ci++)
				ptc[ci] = ptc[ci] / renorm;
		}
	}

	if (renorm == 0.0) {	

		/* 
			Renormalize probabilities 

			This could happen if we get, say, a one-word
			message like 'gurgle:' -- in this case, the code
			above is not reached.
		*/

		log_warnx("## NO SIGNIFICANT HITS FOR ANY CLASS!!!\n## "
		    "(text is %.50s...)\n", p_text);

		for (ci = 0; ci < num_classes; ci++)
			renorm += ptc[ci];

		for (ci = 0; ci < num_classes; ci++)
			ptc[ci] = ptc[ci] / renorm;
	}

#if TRACE_CLASSIFY
	for (ci = 0; ci < num_classes; ci++)
		log_debug(0, "TRACE_CLASSIFY: %s: probability of match for "
		    "file %" PRIu32 ": %f\n",
		    f, ci, ptc[ci]);
#endif

}

